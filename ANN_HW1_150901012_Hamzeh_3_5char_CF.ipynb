{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FOYS74oy49x7",
        "yU8qXJcL5JH3",
        "g_4Ugzyl5M2-",
        "TLHr56FjBk6J",
        "LntWuwrVFCQp",
        "NNj6BNfwP_84",
        "YgmWnvPaRn-B"
      ],
      "authorship_tag": "ABX9TyPLYAgL60FM0JzadEBy3vEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghazaleh99/language_detection_cf/blob/main/ANN_HW1_150901012_Hamzeh_3_5char_CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#تمرین یک درس شبکه های عصبی\n",
        "###**سوال سوم**\n",
        "\n",
        "غزاله حمزه\n",
        "\n",
        "##قسمت الف و ب"
      ],
      "metadata": {
        "id": "jv21Md-IZ7gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1 - Import the library"
      ],
      "metadata": {
        "id": "nxbF6IAPTwso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggdih9wbNofK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2 - Setting up the Data"
      ],
      "metadata": {
        "id": "hWIfm6G4T6ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "langD_file_path = 'ANN-HW1-Data-LangID.xlsx'\n",
        "lang_data = pd.read_excel(langD_file_path)\n",
        "lang_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-ifTG7rTsNj",
        "outputId": "4d969791-0668-4e6b-d328-5038b88e1e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['جملات', 'زبان'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3 - Splitting the Data"
      ],
      "metadata": {
        "id": "xvNTEx3VUTWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size_per = 0.8\n",
        "data_shape = lang_data.shape\n",
        "print('Data shape is',data_shape)\n",
        "\n",
        "train_size = int(data_shape[0] //3 * train_size_per)\n",
        "test_size = int(data_shape[0] //3 - train_size)\n",
        "\n",
        "print('test_size', test_size)\n",
        "print('train_size', train_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c5iw8I7RX4k",
        "outputId": "bb9ad325-4993-4650-8b4a-f9a1281532be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape is (150, 2)\n",
            "test_size 10\n",
            "train_size 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lang_data['جملات'][149])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06l172YnUD8j",
        "outputId": "24bb0ab9-9835-4b31-8348-520d172297d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "گشتی کەش و ھەوایەکی وشکی ھەیە لەگەڵ زستانی سارد و ھاوینی گەرم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split(lang, rA, rB, lang_data, test_size):\n",
        "  train_data = []\n",
        "  test_data = []\n",
        "\n",
        "  rand_list = random.sample(range(rA,rB), test_size) #10 = test size\n",
        "  for i in range(rA,rB):\n",
        "    if i in rand_list:\n",
        "      test_data.append(lang_data['جملات'][i])\n",
        "    else:\n",
        "      train_data.append(lang_data['جملات'][i])\n",
        "\n",
        "  print(lang, 'test data is',test_data)\n",
        "  print('length of', lang, 'test data',len(test_data))\n",
        "  print(lang, 'train data is ',train_data)\n",
        "  print('length of', lang, 'train data',len(train_data))\n",
        "\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "lP5U1mAHwbPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting Arabic dataset"
      ],
      "metadata": {
        "id": "QyE7eoI2fRr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_arb_data, test_arb_data = split('Arabic', 0, 50, lang_data, test_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnmAHDeyx2ph",
        "outputId": "a9e58c00-b3a1-4f4a-ba61-4384f156af75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic test data is ['الجهود مستمرة لإطلاق سراح السفير المصري بالعراق', 'المريخ يكثف الاعداد للمقاولون العرب ويكمل اجراءات السفر', 'سوار الذهب في زيارة الإعلاميين بجدة', 'كلييسترز تحرز لقب لوس أنجلوس للسيدات', 'كما أدين فرانك باغتصاب ثلاثة من أبنائه', 'نانسي عجرم تنفي ضرب محمود سعد لها وتقول لم أشتم الإسلام', 'واعتقل الرجل في منزله في مدينة ماربورج بغرب البلاد السبت', 'وقال مصدر بالشرطة إن الاعتقالات لم تكن تطورا مهما', 'هل يتسبب شداد في تعطيل مشاركة المريخ عربيا', 'يوجد حاليا ضيف ضيوف عضو أعضاء يتصفحون الموقع']\n",
            "length of Arabic test data 10\n",
            "Arabic train data is  [' هل يتسبب شداد في تعطيل مشاركة المريخ عربيا', 'اتحاد الطائرة يرفض الاستقالة', 'ادوات تجميل للرجال تشجعهم للتشبه بالنساء', 'الاتحاد الرياضي يرسل برنامج الدوري العام', 'الارباب يغادر الاسماعيلية الى جده في مهمة خاصة', 'البرازيلي يحتاط للمباراة الثانية', 'التحرير وبري وجها لوجه بدار الرياضة عصر اليوم', 'الحكم بسجن أعضاء عصابة للاعتداءات الجنسية في فرنسا', 'الحكومة الإسرائيلية أقامت وحدات سكنية لاستيعاب المستوطنين', 'الشرطة البريطانية اعتقلت حتى الن شخصا بعد عشرة أيام من التفجيرات الأخيرة', 'العراق مقتل مسلحا في معارك قرب الحدود السورية', 'القرار الإيراني قد يزيد من التصعيد في الملف النووي', 'النائب الاول لرئيس الجمهورية يرحب بزيارة بلاتر ويعلن جاهزية الدولة للدعم', 'الهلال بقولدن قيت والمريخ يكثف الاعداد', 'إسرائيل تهدد برد قاس إذا هوجمت خلال انسحابها من غزة', 'إسرائيل لا تريد تلبية طلبات تسليح الشرطة الفلسطينية لئلا تنتقل للمقاومة', 'إيران اعتقلت العديد من المشتبه بتعاونهم مع مخابرات أجنبية بشأن ملفها النووي', 'أربعة بلدان تسعى لمقعد دائم بمجلس الأمن', 'بي بي سي ليست مسؤولة عن محتويات المواقع الخارجية', 'تستطيع التسجيل مجانا بالضغط', 'تكريم شخصية عربية في الكويت من بينها نانسي عجرم وعمرو خالد', 'خديجة بن قنة تلقت رسالة تهنئة', 'ريكاردو بدايتنا الدورية ليست سهلة', 'شيراك وشرودر اتفقا على عدم رفع ميزانية الاتحاد الاوروبي', 'صور فاضحة لهيفاء وهبي أمام القضاء اللبناني', 'عزمي يسجل في الحارسين وبدر الدين داؤود يبث الرعب', 'عمال اغاثة ينقلون جثة الإسرائيلية القتيلة', 'غارات للجيش على قرى جنوب دارفور', 'فوزي المرضي ترتيباتنا جاهزة لاستئناف التدريبات', 'في افتتاح الدورة الثانية للممتاز الهلال في مواجهة ساخنة امام الخرطوم', 'قادة الأمن الفلسطيني أكدوا استعدادهم لتحمل مسؤولياتهم', 'لجنة للاشراف على الدرع والاكاديمية', 'مدرب المقاولون سنتأهل على حساب المريخ', 'مسؤول حكومي يحذر من قضية ابيي ويقول كير وحدوي ويستطيع التعامل مع المشاكل القبلية', 'واتهم تل أبيب أيضا بسرقة الرمال من أرض المستوطنات', 'والمنتخب يشارك في بطولة الامم الافريقية', 'وزير الصناعة يقف علي حجم خسائر الاحداث الاخيرة بعدد من المصانع', 'وفد من الكونغرس الأمريكي يزور البلاد بالاربعاء', 'وكذلك حكم على أخوين متورطين في العصابة بالسجن عاما', 'يذكر ان هذه اول زيارة يقوم بها بلير الى السعودية منذ شهر اكتوبر تشرين الاول من عام']\n",
            "length of Arabic train data 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting Farsi dataset"
      ],
      "metadata": {
        "id": "ksEYtLV3fd5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_per_data, test_per_data = split('Persain', 50, 100, lang_data, test_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1jf5y-4fQyy",
        "outputId": "8ae69a7d-b7f7-465c-8d5e-c5a60924f30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persain test data is ['مرکز این شهرستان شهر\\xa0مریوان\\xa0است', 'نام کشور در مکاتبات رسمی از پرس و پرشیا به ایران تبدیل شد', 'پایتخت وی شهر\\xa0مشهد\\xa0بود', 'میزان بارندگی در ایران بسیار متغیر است', 'تاجیکستان دارای رودهای بسیاری است', 'حکومت\\xa0سامانی\\xa0نیز از تاجیکستان برخاسته\\u200cاست', 'خراسان\\xa0و\\xa0فرارود\\xa0مهد تمدن و علوم ایرانی بوده است', 'در آسیای میانه دین اسلام چاری شده زبان عربی عمومی گردید', 'استیلای عرب\\u200cها به آسیای میانه دگرگونی\\u200cهای زیادی آورد', 'حرف آ در اسپانیایی در تمام کلمات بدون تغییری به همان صورت خوانده می\\u200cشود']\n",
            "length of Persain test data 10\n",
            "Persain train data is  ['آلبرت حدوداً یک ساله بود که خانواده او به شهر مونیخ نقل مکان کرد', 'او عاقبت در سن سه سالگی شروع به حرف زدن کرد', 'معلوم بود که آلبرت آنگونه که پدرش می\\u200cخواست\\xa0مهندس الکترونیک\\xa0نخواهد شد', 'ایران در منطقه موقعیتی راهبردی دارد', 'همچنین بزرگترین ذخیره فلز\\xa0روی\\xa0در جهان است', '\\xa0فرهنگ ابزارسازی این دوره تیغه و ریزتیغه برادوستی است', 'بقایای\\xa0انسان\\u200cهای نئاندرتال\\xa0نیز در ایران به ویژه در زاگرس پیدا شده\\u200cاست', 'او سرکردهٔ طایفه پاسارگاد از طایفه\\u200cهای پارسیان بوده\\u200cاست', 'پادشاهی هخامنشیان تبدیل به شاهنشاهی بزرگی شد', 'شهر\\xa0تیسفون\\xa0در نزدیکی\\xa0بغداد\\xa0در عراق امروزی بود', 'قیام\\u200cهایی از جمله خیزش\\xa0المقنع\\xa0نیز علیه عباسیان صورت گرفت', 'طاهریان اولین حکومت مستقل ایران پس از\\xa0حملهٔ اعراب\\xa0بودند', 'صَفّاریان از دودمان\\u200cهای ایرانی فرمانروای بخش\\u200cهایی از ایران بودند', 'آن\\u200cها توانستند به حکومت ایران غربی برسند', 'حکومت این خانواده\\u200cها به نام\\xa0دیلمیان\\xa0شهرت یافته\\u200cاست', 'بوییان دارای تباری دیلمی بودند', 'اما\\xa0سلطان محمود\\xa0او را گرفت و در هند زندانی کرد', 'اتسز نتوانست به توسعه قلمرو خوارزمشاهیان کمک چندانی بکند', 'چنگیزخان به مغولستان بازگشت و در آنجا درگذشت', '\\xa0ایلخانان در ابتدا دین بودایی داشتند', '\\xa0ایلخانان مسلمان خود\\xa0 را سلطان نامیده و نام\\u200cهای اسلامی برگزیدند', 'با مرگ او بیشتر متصرفاتش سر به شورش برداشتند', 'دوره صفویه از مهم\\u200cترین دوران تاریخی ایران به شمار می\\u200cآید', 'آن\\u200cها می\\u200cخواهند ایران را مانند\\xa0هندوستان\\xa0کنند', 'آنان ابتدا در پیرامون ارمنستان ساکن شدند', 'ملی شدن صنعت نفت مهم\\u200cترین رویداد دوران وی محسوب می\\u200cشود', 'سلطنت پهلوی با\\xa0انقلاب ایران\\xa0برچیده شد', 'بیش از نیمی از ایران کویری و نیمه کویری است', 'البرز\\xa0آخرین استانی بود که\\xa0 تاکنون تأسیس شده\\u200cاست', 'شمالی\\u200cترین شهر\\xa0پارس\\u200cآباد\\xa0و جنوبی\\u200cترین شهر\\xa0چابهار\\xa0است', 'حدود یک سوم ایران کوهستانی است', 'ایران از لحاظ آب و هوایی یکی از منحصربه\\u200cفردترین کشورهاست', 'سیستم کم فشار جنوبی که در نوار جنوب و جنوب غرب موجب رگبار باران می\\u200cشود', 'در مورد ترکیب جمعیتی اقوام در ایران گزارش\\u200cهای مختلفی منتشر شده\\u200cاند', 'مردم تاجیک\\xa0دین اسلام\\xa0را پذیرفتند', 'امامعلی رحمان\\xa0ریاست جمهوری\\xa0تاجیکستان را به عهده دارد', 'نظم و نثر فارسی و تاجیکی در طول چندین عصرها ترقی می\\u200cکرد', 'هم\\u200cاکنون در کشور\\xa0تاجیکستان\\xa0رسماً از خط سیریلیک\\xa0 استفاده می\\u200cشود', 'قبل از انقلاب روسیه خط مردم تاجیکستان پارسی بود', 'نام کتاب او\\xa0اسطورشیا\\xa0به معنای اصول هندسه\\u200cاست']\n",
            "length of Persain train data 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting Kordi dataset"
      ],
      "metadata": {
        "id": "Rvxkd8IzfgqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_kor_data, test_kor_data = split('Kordish', 100, 150, lang_data, test_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQXnCtU2fQ9Q",
        "outputId": "f1a0671a-3b7a-4227-b37f-43e258ff2fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kordish test data is ['\\xa0سوێد و\\xa0دانماڕک\\xa0بەیەکەوە بەستراونەتەوە لە ڕێگای پردەھۆینی ئۆریسەند', 'باکوور و ڕۆژاوای\\xa0عێراقە\\xa0و و باشووریشی\\xa0سعوودیەیە', 'چه\\u200cن خاڵی وه\\u200cرچه\\u200cرخان له\\u200c مێژووی زانستی زمانناسیدا به\\u200cدی ده\\u200cکرێت', 'دەتوانی یارمەتیی ویکیپیدیا بدەیت بە\\xa0فراوانکردنی', 'زمانی عەرەبی سەربە خێزانی\\xa0زمانە سامییەکانە\\xa0کە بەشێکە لە زمانە\\xa0ئەفرۆئاسیاییەکان', 'فارسیی نوێ\\xa0رۆڵەی فارسیی کۆنە', 'نیسانەکانی ئەم زمانە ئاماژەیە و زۆرتر لە کەسانی نابینا و نابیسادا بە کاردێت', 'هەروەها لە ژێر کاریگەرییەکی زۆری کلتووری چینی و هیندییە', 'ھیندستان بەوڵاتێکی فرە\\xa0زمان\\xa0و\\xa0ئایین\\xa0و\\xa0چاند\\xa0دەناسرێتەوە', 'ئەم زمانە لە زمانی کەڤناری\\xa0مادی\\xa0کەوتووەتەوە']\n",
            "length of Kordish test data 10\n",
            "Kordish train data is  ['ایتەختەکەی\\xa0شاری کووەیتە', 'ایتەختەکەی\\xa0وەڵینگتۆنە', 'بە گواستنەوەی پەیامەکان لە ڕێگای جووڵەی ئەندامەکانی لەش دەڵێن', 'بەڵام پلانیش ھەیە کە بە دیالێکتەکانی\\u200c زازا و سۆرانیش بەرنامەی\\u200c ھەبێ\\u200c', 'بەوەی\\u200c چەند نەتەوە ھەن لە تورکیەدا', 'پاشتونەکان زیاد لە نیوەی دانیشتوان پێکدەھێنن', 'پێویستە بوترێ کە ئوسترالیا پێنجەم وڵاتی جیهانە لە ڕووێ پانتاییەوە', 'تاجیکەکان نزیکەی یەک لەسەر پێنجی دانیشتوان پێکدەھێنن', 'جگە لەچەندین لەنگەری ئاوی لە ماڵمۆ و گۆتەنبێڕگ', 'دەکۆڵێتەوە لە پێکهاتن و دروستکردنی ڕستە', 'رۆژنامه\\u200cی\\u200c شه\\u200cرقولئه\\u200cوسه\\u200cت به\\u200c زمانی عه\\u200cره\\u200cبی ده\\u200cرده\\u200cچێ و ماڵپه\\u200cره\\u200cکه\\u200cی به\\u200c زمانی ئینگلیزیه\\u200c', 'زمانی فەرمی ووڵات\\xa0زمانی عەرەبییە\\xa0وە\\xa0زمانی ئینگلیزی\\xa0زمانی دووەمە', 'زمانی کوردی\\xa0زمانێکە کە خەڵکی\\xa0کورد\\xa0قسەی پیدەکەن', 'زۆربەیان دانیشتوانی ئازەربایجان\\xa0ئازەرین', 'زۆرینەی دانیشتوانی نیوزلاند کۆچبەرە ئەوروپیەکانن بە تایبەتی بەریتانی و ئیرلەندی', 'سیدنی\\xa0و\\xa0مێلبۆرن\\xa0لەدوای یەک دوو\\xa0گەورە شاری\\xa0ئوسترالیان', 'سەرۆک دەتوانێت سەرۆکوەزیران ھەڵبژێرێت کە ئەوییان زۆرترین دەسەڵاتی ھەیە', 'عه\\u200cره\\u200cب نیوز وسعودی گازێتیش به\\u200c زمانی ئینگلیزی بڵاو ده\\u200cکرێنه\\u200cوه\\u200c', 'کە بە یەکەم دەوڵەتی سەربەخۆی ئەو ناوچەیە دادەندرێت', 'لقێکی زمانناسییە کە لە واتای وشە دەکۆڵێتەوە', 'لە بیرمان بێ کە کاری زمانەوان ناساندنە ، نەوەک دانان', 'لە پێکهاتن و دروستبوونی وشە دەکۆڵێتەوە', 'لە تێڕوانینی زەویناسانەوە کۆنترین وشکی جیهان هەژمار دەکرێ', 'لە ڕۆژھەڵاتی ئەم پارێزگایە ژمارەیەکی کەم\\xa0ئازەریش\\xa0ئەژین', 'مالیزیا خاوەن کۆمەڵگەیەکی فرەنەتەوە، فرەئایین و فرەزمانە', 'مالیزیا خاوەن یەک لە بەهێزترین ئابوورییەکانی ئاسیایە', 'مرۆڤ گیاندارێکی کۆمەڵژییە و زمان وەک پێداویستییەکی گرنگ بۆ پێوەندی گرتنی دەوری ھەیە', 'ناوەندی ئەم شارستانە\\xa0شاری مەریوانە', 'نیشانەکانی ئەم زمانە خەتییە، فێرخواز پێویستە لە قوتابخانە فێری ببێ', 'وەبەشێکی ڕۆژئاوای وڵاتەکە دەکەوێتە سەر\\xa0دەریای قەزوین', 'هەر هەرێمێک دەسەڵاتدارێک بەرێوەی دەبات', 'ئوسترالیا کە وەکوو دوورگەیەکی گەورە لە\\xa0ئۆقیانووسی ئارام\\xa0دا هەڵکەوتووە', 'ئێران بە ھۆی ھەبوونی لە ناوچەی\\xa0ئەوراسیا\\xa0دا جێگایەکی تایبەتی ھەیە', 'ئێران یەکێکە لەو وڵاتانەی کە بەرزترین ڕێژەی گەشەکردنی شارەکانی هەیە', 'ئەفغانستان سنووری بەستووە لەگەڵ وڵاتانی\\xa0ئێران،\\xa0پاکستان،\\xa0ئوزبەکستان\\xa0و\\xa0تاجیکستان', 'ئەم جۆرە زمانناسیە تا پەنجا ساڵ دریژەی سەند', 'ئەم ڕستەیە وەک پێکھاتەیەکی دەقەکە، چەندە لە پێکھێنانی دەقەکەدا کاریگەرە', 'ئەم کردارانەی ئاماژە بە ئەنجام بوونی کارێک لە ڕابردوو دەکەن', 'ئەو بەشەی زمانناسییە کە لە واتا و بەکارهێنانی دەکۆڵێتەوە', 'گشتی کەش و ھەوایەکی وشکی ھەیە لەگەڵ زستانی سارد و ھاوینی گەرم']\n",
            "length of Kordish train data 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4 - Train"
      ],
      "metadata": {
        "id": "bpqS5Y5QpPqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4 - Train with 5 char"
      ],
      "metadata": {
        "id": "7xNRC0dLAKkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_alphabet = {\n",
        "    0 : 'پ',\n",
        "    1 : 'ژ',\n",
        "    2 : 'گ',\n",
        "    3 : 'چ',\n",
        "    4 : 'ع',\n",
        "}"
      ],
      "metadata": {
        "id": "kqMEw5m2q_FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AVGtreshold(lang, train_data, feature_alphabet):\n",
        "  means_lang = []\n",
        "  for i in train_data:\n",
        "    sum = 0\n",
        "    i = i.replace('\\xa0','')\n",
        "    i = i.replace('\\u200c','')\n",
        "    i = i.replace('،','')\n",
        "    i = i.replace(' ','') #removing the space\n",
        "    for j in feature_alphabet.values():\n",
        "      if j in i:\n",
        "        sum += 1\n",
        "    mean = sum/len(i)\n",
        "    means_lang.append(mean)\n",
        "\n",
        "  sum = 0\n",
        "  for i in means_lang:\n",
        "    sum += i\n",
        "\n",
        "  Avg_lang = sum/len(means_lang)\n",
        "  print('avarage for', lang, 'is', Avg_lang)\n",
        "  return Avg_lang"
      ],
      "metadata": {
        "id": "A2nr6HxxzFmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Treshold for arabic is :"
      ],
      "metadata": {
        "id": "5zHdrdlCuCUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Avg_arab = AVGtreshold('Arabic', train_arb_data, feature_alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSBL7wAvrz1a",
        "outputId": "218192be-2b4d-45c3-b98f-2aba5653b62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avarage for Arabic is 0.018085165726954268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Treshold for Persain is :"
      ],
      "metadata": {
        "id": "6du-Y9F2ua24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Avg_per = AVGtreshold('Persain', train_per_data, feature_alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X7RPIWPubIp",
        "outputId": "831db2ed-61e5-42b2-ec3a-dca10c1889f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avarage for Persain is 0.026083172820938305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Treshold for Kordish is :"
      ],
      "metadata": {
        "id": "ne0OlLGhu1ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Avg_kor = AVGtreshold('Kordish', train_kor_data, feature_alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOHKv0LXu3oW",
        "outputId": "3ba397aa-1d86-4341-8fcd-351c7ab7d40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avarage for Kordish is 0.02586788527430186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4 - Train with Character Frequency (CF)"
      ],
      "metadata": {
        "id": "i9hRQYf2AccI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Preprocessing\n",
        "removing space char from all data"
      ],
      "metadata": {
        "id": "wgsjpq4aAtdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_alphabet = []\n",
        "for i in train_arb_data:\n",
        "  for j in i:\n",
        "    if j not in bag_alphabet:\n",
        "      bag_alphabet.append(j)\n",
        "\n",
        "for i in train_per_data:\n",
        "  for j in i:\n",
        "    if j not in bag_alphabet:\n",
        "      bag_alphabet.append(j)\n",
        "\n",
        "for i in train_kor_data:\n",
        "  for j in i:\n",
        "    if j not in bag_alphabet:\n",
        "      bag_alphabet.append(j)\n",
        "\n",
        "bag_alphabet.remove(' ')\n",
        "bag_alphabet.remove('\\xa0')\n",
        "bag_alphabet.remove('\\u200c')\n",
        "bag_alphabet.remove('،')\n",
        "\n",
        "print(bag_alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y66wUiz4Anjo",
        "outputId": "f717e21d-1542-4ab6-db8b-dd3f3805cfed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ه', 'ل', 'ي', 'ت', 'س', 'ب', 'ش', 'د', 'ا', 'ف', 'ع', 'ط', 'م', 'ر', 'ك', 'ة', 'خ', 'ح', 'ئ', 'ض', 'ق', 'و', 'ج', 'ن', 'ء', 'غ', 'ى', 'ص', 'ز', 'ث', 'أ', 'إ', 'ذ', 'ؤ', 'آ', 'ً', 'ی', 'ک', 'گ', 'پ', 'چ', 'ژ', 'ٔ', 'َ', 'ّ', 'ظ', 'ە', 'ڵ', 'ۆ', 'ڕ', 'ێ', 'ھ', 'ڤ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Character Frequency (CF)"
      ],
      "metadata": {
        "id": "_2Bfd4GFBW_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_temp(bag_alphabet):\n",
        "  charFrequency_lang = {}\n",
        "  charFrequency = {}\n",
        "  for i in bag_alphabet:\n",
        "    if i not in charFrequency_lang.keys():\n",
        "      charFrequency_lang[i] = [] #list of repeatation for evrey sentence\n",
        "      charFrequency[i] = int(0) \n",
        "  # print(len(charFrequency))\n",
        "  return charFrequency, charFrequency_lang"
      ],
      "metadata": {
        "id": "3CDiCwlWm6pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def charFC(t_data, bag_alphabet):\n",
        "  charFrequency, charFrequency_lang = dict_temp(bag_alphabet)\n",
        "  for i in t_data:\n",
        "    i = i.replace(' ','')\n",
        "    i = i.replace('\\xa0','')\n",
        "    i = i.replace('\\u200c','')\n",
        "    i = i.replace('،','')\n",
        "    # print(i)\n",
        "    for j in i:\n",
        "      if j in charFrequency.keys():\n",
        "        charFrequency[j] += int(1)\n",
        "    for k in charFrequency.keys():\n",
        "      charFrequency_lang[k].append(charFrequency[k])\n",
        "  return charFrequency_lang"
      ],
      "metadata": {
        "id": "vfx9E_q3CLiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train for Arabic, Persain, Kordish"
      ],
      "metadata": {
        "id": "YrSzNAFyEBkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "charFrequency_arb = charFC(train_arb_data, bag_alphabet)\n",
        "charFrequency_per = charFC(train_per_data, bag_alphabet)\n",
        "charFrequency_kor = charFC(train_kor_data, bag_alphabet)\n",
        "# print(len(charFrequency_kor['ڤ']))\n",
        "# print(len(charFrequency_per['ڤ']))\n",
        "# print(len(charFrequency_arb['ڤ']))"
      ],
      "metadata": {
        "id": "nmDH7ZslDejL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Computing the normal CF"
      ],
      "metadata": {
        "id": "w7JwFwnoD824"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalCF(charFrequency_lang, bag_alphabet, train_size):\n",
        "  charFrequency, lang_normalCF = dict_temp(bag_alphabet)\n",
        "  for i in charFrequency.keys():\n",
        "    for j in range(train_size):\n",
        "      ncf = charFrequency_lang[i][j]/len(charFrequency_lang[i])\n",
        "      lang_normalCF[i].append(ncf)\n",
        "\n",
        "  lang_sumNFC = charFrequency.copy()\n",
        "  for i in charFrequency.keys():\n",
        "    tmp = 0\n",
        "    for j in range(train_size):\n",
        "      tmp += lang_normalCF[i][j]\n",
        "    lang_sumNFC[i] = tmp/train_size\n",
        "  print(lang_sumNFC)\n",
        "  \n",
        "  return lang_sumNFC"
      ],
      "metadata": {
        "id": "gAcxK8zpD6gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arb_sumNFC = normalCF(charFrequency_arb, bag_alphabet, train_size)\n",
        "per_sumNFC = normalCF(charFrequency_per, bag_alphabet, train_size)\n",
        "kor_sumNFC = normalCF(charFrequency_kor, bag_alphabet, train_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ufytdq2FSEI",
        "outputId": "6fdf6f5e-f51e-44d6-d7b6-49adc1e3636b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ه': 0.40625, 'ل': 2.52375, 'ي': 1.8581249999999998, 'ت': 1.0856249999999998, 'س': 0.6475000000000001, 'ب': 0.8837499999999998, 'ش': 0.25750000000000006, 'د': 0.7931250000000001, 'ا': 3.38125, 'ف': 0.36562500000000003, 'ع': 0.6893749999999998, 'ط': 0.22875000000000006, 'م': 1.0618750000000001, 'ر': 1.2968749999999998, 'ك': 0.20500000000000002, 'ة': 0.8387500000000001, 'خ': 0.21312499999999995, 'ح': 0.33874999999999994, 'ئ': 0.17187500000000006, 'ض': 0.14187500000000006, 'ق': 0.38499999999999995, 'و': 0.769375, 'ج': 0.4231250000000001, 'ن': 0.8256250000000002, 'ء': 0.08687499999999998, 'غ': 0.070625, 'ى': 0.09562500000000002, 'ص': 0.13062499999999994, 'ز': 0.138125, 'ث': 0.07499999999999997, 'أ': 0.18, 'إ': 0.10999999999999995, 'ذ': 0.023750000000000007, 'ؤ': 0.03375, 'آ': 0.0, 'ً': 0.0, 'ی': 0.0, 'ک': 0.0, 'گ': 0.0, 'پ': 0.0, 'چ': 0.0, 'ژ': 0.0, 'ٔ': 0.0, 'َ': 0.0, 'ّ': 0.0, 'ظ': 0.0, 'ە': 0.0, 'ڵ': 0.0, 'ۆ': 0.0, 'ڕ': 0.0, 'ێ': 0.0, 'ھ': 0.0, 'ڤ': 0.0}\n",
            "{'ه': 1.3587500000000001, 'ل': 0.5575, 'ي': 0.0, 'ت': 0.9975000000000002, 'س': 0.78625, 'ب': 0.9281250000000002, 'ش': 0.5225, 'د': 1.628125, 'ا': 3.0631250000000003, 'ف': 0.2699999999999999, 'ع': 0.22687500000000008, 'ط': 0.12499999999999996, 'م': 0.91625, 'ر': 1.7887500000000003, 'ك': 0.0, 'ة': 0.0, 'خ': 0.29562499999999997, 'ح': 0.16375, 'ئ': 0.021250000000000012, 'ض': 0.0, 'ق': 0.22437500000000005, 'و': 1.236875, 'ج': 0.10187499999999998, 'ن': 1.910625, 'ء': 0.0, 'غ': 0.09874999999999998, 'ى': 0.0, 'ص': 0.07749999999999999, 'ز': 0.5037499999999999, 'ث': 0.0025, 'أ': 0.0075, 'إ': 0.0, 'ذ': 0.04000000000000001, 'ؤ': 0.0, 'آ': 0.15500000000000008, 'ً': 0.026875000000000017, 'ی': 2.0000000000000004, 'ک': 0.48, 'گ': 0.26312499999999994, 'پ': 0.15499999999999997, 'چ': 0.06874999999999999, 'ژ': 0.021250000000000012, 'ٔ': 0.03875000000000002, 'َ': 0.01750000000000001, 'ّ': 0.01750000000000001, 'ظ': 0.008124999999999999, 'ە': 0.0, 'ڵ': 0.0, 'ۆ': 0.0, 'ڕ': 0.0, 'ێ': 0.0, 'ھ': 0.0, 'ڤ': 0.0}\n",
            "{'ه': 0.456875, 'ل': 0.5693750000000001, 'ي': 0.0, 'ت': 1.036875, 'س': 0.438125, 'ب': 0.5956249999999998, 'ش': 0.33062500000000006, 'د': 0.828125, 'ا': 2.1456249999999994, 'ف': 0.06687499999999998, 'ع': 0.06562500000000003, 'ط': 0.0, 'م': 0.6237500000000001, 'ر': 1.0775000000000001, 'ك': 0.0, 'ة': 0.0, 'خ': 0.12249999999999998, 'ح': 0.0, 'ئ': 0.38375, 'ض': 0.0, 'ق': 0.074375, 'و': 1.6275, 'ج': 0.17250000000000001, 'ن': 1.9950000000000006, 'ء': 0.0, 'غ': 0.00375, 'ى': 0.0, 'ص': 0.0, 'ز': 0.5818749999999999, 'ث': 0.0, 'أ': 0.0, 'إ': 0.0, 'ذ': 0.0, 'ؤ': 0.0, 'آ': 0.0, 'ً': 0.0, 'ی': 2.6618750000000007, 'ک': 1.06, 'گ': 0.3175, 'پ': 0.32687499999999997, 'چ': 0.09874999999999998, 'ژ': 0.10187500000000001, 'ٔ': 0.0, 'َ': 0.0, 'ّ': 0.0, 'ظ': 0.0, 'ە': 3.4456250000000006, 'ڵ': 0.3581250000000001, 'ۆ': 0.354375, 'ڕ': 0.121875, 'ێ': 0.7212500000000002, 'ھ': 0.18249999999999994, 'ڤ': 0.00875}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5 - Test"
      ],
      "metadata": {
        "id": "_B7qJTViwVoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5 - Test with 5 char"
      ],
      "metadata": {
        "id": "8dc7JL6HASGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_means(test_data, feature_alphabet):\n",
        "  means_lang = []\n",
        "  for i in test_data:\n",
        "    sum = 0\n",
        "    sent = i.replace(' ','') #removing the space\n",
        "    for j in feature_alphabet.values():\n",
        "      if j in sent:\n",
        "        sum += 1\n",
        "    mean = sum/len(sent)\n",
        "    means_lang.append(mean)\n",
        "\n",
        "  return means_lang"
      ],
      "metadata": {
        "id": "fP1YKudv06Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_result(lang, test_means, Avg_arab, Avg_per, Avg_kor):\n",
        "  test_result = []\n",
        "  error_arab, error_per, error_kor = 0, 0, 0\n",
        "  for i in test_means:\n",
        "    tmp = pow(Avg_arab - i, 2)\n",
        "    error_arab += tmp\n",
        "    tmp = pow(Avg_per - i, 2)\n",
        "    error_per += tmp\n",
        "    tmp = pow(Avg_kor - i, 2)\n",
        "    error_kor += tmp\n",
        "\n",
        "    error_arab = math.sqrt(error_arab)\n",
        "    error_per = math.sqrt(error_per)\n",
        "    error_kor = math.sqrt(error_kor)\n",
        "\n",
        "    min_error = min(error_arab, error_per, error_kor)\n",
        "    if min_error == error_arab:\n",
        "      test_result.append('Arabic')\n",
        "    elif min_error == error_per:\n",
        "      test_result.append('Persain')\n",
        "    else:\n",
        "      test_result.append('Kordish')\n",
        "\n",
        "  print('Test result for', lang, test_result)\n",
        "  return test_result"
      ],
      "metadata": {
        "id": "-4IS3J5p3wzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Test result for arabic is :"
      ],
      "metadata": {
        "id": "FOYS74oy49x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_means_arab = test_means(test_arb_data, feature_alphabet)\n",
        "test_result_arab1 = test_result('Arabic', test_means_arab, Avg_arab, Avg_per, Avg_kor)"
      ],
      "metadata": {
        "id": "Dx6TbhbDydDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a023ef-e340-400e-b768-3820960afda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test result for Arabic ['Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Test result for Persain is :"
      ],
      "metadata": {
        "id": "yU8qXJcL5JH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_means_per = test_means(test_per_data, feature_alphabet)\n",
        "test_result_per1 = test_result('Persain', test_means_per, Avg_arab, Avg_per, Avg_kor)"
      ],
      "metadata": {
        "id": "hzyajgltyamP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa9e7bd-e203-4837-d311-0b506e4aa836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test result for Persain ['Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Test result for Kordish is :"
      ],
      "metadata": {
        "id": "g_4Ugzyl5M2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_means_kor = test_means(test_kor_data, feature_alphabet)\n",
        "test_result_kor1 = test_result('Kordish', test_means_kor, Avg_arab, Avg_per, Avg_kor)"
      ],
      "metadata": {
        "id": "3m942EdvwQCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d512bff9-3034-4fd6-ce4e-5506dc15aa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test result for Kordish ['Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5 - Test with with Character Frequency (CF) "
      ],
      "metadata": {
        "id": "1hZ2bKBlIMwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ncf_testing(charFrequency_lang, bag_alphabet, test_size):\n",
        "  charFrequency, lang_normalCF = dict_temp(bag_alphabet)\n",
        "  for i in charFrequency.keys():\n",
        "    for j in range(test_size):\n",
        "      ncf = charFrequency_lang[i][j]/len(charFrequency_lang[i])\n",
        "      lang_normalCF[i].append(ncf)\n",
        "  return lang_normalCF"
      ],
      "metadata": {
        "id": "l6fe6ZXSmEQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(ncf_test, ncf_arb, ncf_per, ncf_kor):\n",
        "  arb_Euclidean, per_Euclidean, kor_Euclidean = 0, 0, 0\n",
        "  for i in ncf_test.keys():\n",
        "    # print(ncf_arb[i],ncf_test[i])\n",
        "    tmp = pow(ncf_arb[i] - ncf_test[i], 2)\n",
        "    arb_Euclidean += tmp\n",
        "    tmp = pow(ncf_per[i] - ncf_test[i], 2)\n",
        "    per_Euclidean += tmp\n",
        "    tmp = pow(ncf_kor[i] - ncf_test[i], 2)\n",
        "    kor_Euclidean += tmp\n",
        "\n",
        "  arb_Euclidean = math.sqrt(arb_Euclidean)\n",
        "  per_Euclidean = math.sqrt(per_Euclidean)\n",
        "  kor_Euclidean = math.sqrt(kor_Euclidean)\n",
        "\n",
        "  print(arb_Euclidean, per_Euclidean, kor_Euclidean)\n",
        "  \n",
        "  tmp = min(arb_Euclidean, per_Euclidean, kor_Euclidean)\n",
        "  if tmp == arb_Euclidean:\n",
        "    result = 'Arabic'\n",
        "  elif tmp == per_Euclidean:\n",
        "    result = 'Persain'\n",
        "  elif tmp == kor_Euclidean:\n",
        "    result = 'Kordish'\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "3X210vUkqQFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resultFunc(test_data, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC):\n",
        "  charFrequency, lang_normalCF = dict_temp(bag_alphabet)\n",
        "  charFrequency_tmp = charFC(test_data, bag_alphabet)\n",
        "  ncf_test_tmp = ncf_testing(charFrequency_tmp, bag_alphabet, len(test_data))\n",
        "  \n",
        "  test_result_list = []\n",
        "  for i in range(len(test_data)):\n",
        "    charFrequencytest = charFrequency.copy()\n",
        "    for j in ncf_test_tmp.keys():\n",
        "      charFrequencytest[j] = ncf_test_tmp[j][i]\n",
        "    tmp = predict(charFrequencytest, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "    test_result_list.append(tmp)\n",
        "  return test_result_list"
      ],
      "metadata": {
        "id": "d5hwwNCCJ057"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result_arab2 = resultFunc(test_arb_data, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "print(test_result_arab2)\n",
        "test_result_per2 = resultFunc(test_per_data, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "print(test_result_per2)\n",
        "test_result_kor2 = resultFunc(test_kor_data, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "print(test_result_kor2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI3IWJCxsmqL",
        "outputId": "0dc6095b-ff02-414f-c135-c7c8f9dd4014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.44139573712476 4.828013955033685 5.700650285000389\n",
            "3.1061223565323375 4.1700382192493155 5.434258336873119\n",
            "2.3510627583605674 3.978563654134492 5.451918347873068\n",
            "1.8329951155826902 4.052803813411156 5.576785693558163\n",
            "1.270677415298627 3.8670361712815673 5.608768908760193\n",
            "1.261495578172987 4.092046401252068 5.938647882462388\n",
            "2.087401756670239 4.759408445384785 6.619585611794368\n",
            "3.0612082735008412 5.425135366974727 7.2867972163272805\n",
            "3.7787889718466676 6.096174517679099 7.943207077237443\n",
            "4.464400418169275 6.7517844863413705 8.506889188879507\n",
            "['Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic']\n",
            "5.050519388513423 4.865744418894195 5.607164048953357\n",
            "4.341427886508078 3.731433203207583 4.904517170107064\n",
            "4.251572778837262 3.3640331077443344 4.713720788493418\n",
            "4.036783508407406 2.515992597365899 4.361010625058714\n",
            "4.046603031401771 1.9115030081064484 4.233014726158533\n",
            "4.231547718477248 1.7798437431415153 4.327388204434056\n",
            "4.530314127491603 1.8311113428735017 4.620258507039948\n",
            "5.356514360454007 2.5975409043939997 5.195121622433397\n",
            "6.3858629873925405 3.7039632490077428 5.989170532876402\n",
            "7.631013765794817 5.106169675010809 7.125783372505439\n",
            "['Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain', 'Persain']\n",
            "5.081866890597392 4.728817901970851 4.613896257164329\n",
            "4.902027243268851 4.177630159552184 3.7914263373926977\n",
            "4.63072576317687 3.1715680270175515 2.9881287910454937\n",
            "5.0539460912983625 2.9054205805700497 2.207411079041465\n",
            "6.050609150635165 3.887186482534637 1.6020810441032627\n",
            "6.553615116388054 4.317591197647133 2.118174608448274\n",
            "7.730580255954271 5.593989520011634 3.646681871492905\n",
            "9.007815833694092 6.828943457812489 4.898996190228667\n",
            "10.000387297187546 7.8608821864979 6.025801911104861\n",
            "10.952887568753274 8.93042377213982 6.993615922530704\n",
            "['Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish', 'Kordish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step6 - Accuracy / F1-measure / Recall / Precision"
      ],
      "metadata": {
        "id": "vRAQYUGZ5VJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Accuracy = TN + TP / Total"
      ],
      "metadata": {
        "id": "TLHr56FjBk6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(test_result_arab, test_result_per, test_result_kor):\n",
        "  total = 30\n",
        "  Tpos1, Tpos2, Tpos3 = 0, 0, 0\n",
        "  Tneg1, Tneg2, Tneg3 = 0, 0, 0\n",
        "\n",
        "  for i in test_result_arab:\n",
        "    if i == 'Arabic':\n",
        "      Tpos1 += 1\n",
        "      Tneg3 += 1\n",
        "      Tneg2 += 1\n",
        "    elif i == 'Persain':\n",
        "      Tneg3 += 1\n",
        "    else:\n",
        "      Tneg2 += 1\n",
        "\n",
        "  for i in test_result_per:\n",
        "    if i == 'Persain':\n",
        "      Tpos2 += 1\n",
        "      Tneg3 += 1\n",
        "      Tneg1 += 1\n",
        "    elif i == 'Arabic':\n",
        "      Tneg3 += 1\n",
        "    else:\n",
        "      Tneg1 += 1\n",
        "\n",
        "  for i in test_result_kor:\n",
        "    if i == 'Kordish':\n",
        "      Tpos3 += 1\n",
        "      Tneg1 += 1\n",
        "      Tneg2 += 1\n",
        "    elif i == 'Persain':\n",
        "      Tneg1 += 1\n",
        "    else:\n",
        "      Tneg2 += 1\n",
        "\n",
        "  accuracy_arab = (Tpos1+Tneg1)/total\n",
        "  accuracy_per = (Tpos2+Tneg2)/total\n",
        "  accuracy_kor = (Tpos3+Tneg3)/total\n",
        "\n",
        "  print('Accuracy:')\n",
        "  print('Accuracy for Arabic is', accuracy_arab)\n",
        "  print('Accuracy for Persain is', accuracy_per)\n",
        "  print('Accuracy for Kordish is', accuracy_kor)\n",
        "\n",
        "  return Tpos1, Tpos2, Tpos3"
      ],
      "metadata": {
        "id": "G0R5iJur59AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Recall = TP/TP+FN\n"
      ],
      "metadata": {
        "id": "LntWuwrVFCQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(Tpos1, Tpos2, Tpos3):\n",
        "  recall_arab = Tpos1/10 #10 - FN = TP\n",
        "  recall_per = Tpos2/10\n",
        "  recall_kor = Tpos3/10\n",
        "  print('Recall:')\n",
        "  print('Recall for Arabic is', recall_arab)\n",
        "  print('Recall for Persain is', recall_per)\n",
        "  print('Recall for Kordish is', recall_kor)\n",
        "\n",
        "  return recall_arab, recall_per, recall_kor"
      ],
      "metadata": {
        "id": "7slljwb-FdRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Precision = TP / TP + FP"
      ],
      "metadata": {
        "id": "NNj6BNfwP_84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def percision(test_result_arab, test_result_per, test_result_kor):\n",
        "  total = 30\n",
        "  Tpos1, Tpos2, Tpos3 = 0, 0, 0\n",
        "  fpos1, fpos2, fpos3 = 0, 0, 0\n",
        "\n",
        "  for i in test_result_arab:\n",
        "    if i == 'Arabic':\n",
        "      Tpos1 += 1\n",
        "    elif i == 'Persain':\n",
        "      fpos2 += 1\n",
        "    else:\n",
        "      fpos3 += 1\n",
        "\n",
        "  for i in test_result_per:\n",
        "    if i == 'Persain':\n",
        "      Tpos2 += 1\n",
        "    elif i == 'Arabic':\n",
        "      fpos1 += 1\n",
        "    else:\n",
        "      fpos3 += 1\n",
        "\n",
        "  for i in test_result_kor:\n",
        "    if i == 'Kordish':\n",
        "      Tpos3 += 1\n",
        "    elif i == 'Persain':\n",
        "      fpos2 += 1\n",
        "    else:\n",
        "      fpos1 += 1\n",
        "\n",
        "  # print('Tpos1+fpos1', Tpos1+fpos1)\n",
        "  precision_arab = Tpos1/(Tpos1+fpos1)\n",
        "  precision_per = Tpos2/(Tpos2+fpos2)\n",
        "  precision_kor = Tpos3/(Tpos3+fpos3)\n",
        "\n",
        "  print('Precision:')\n",
        "  print('Precision for Arabic is', precision_arab)\n",
        "  print('Precision for Persain is', precision_per)\n",
        "  print('Precision for Kordish is', precision_kor)\n",
        "\n",
        "  return precision_arab, precision_per, precision_kor"
      ],
      "metadata": {
        "id": "0Pk9oVIQ8Mkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####F1-Measure = 2 * precision * recall / ( precision + recall )"
      ],
      "metadata": {
        "id": "YgmWnvPaRn-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fMeasure(lang, precision_lang, recall_lang):\n",
        "  FMeasure_lang = 2 * precision_lang * recall_lang / (precision_lang + recall_lang)\n",
        "  print('F1-Measure for', lang, 'is', FMeasure_lang)\n",
        "  return FMeasure_lang"
      ],
      "metadata": {
        "id": "DpwvNRZ285zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 6.2 - Evaluate  "
      ],
      "metadata": {
        "id": "dqcIaNU92jtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 6.2.1 - Evaluate with 5 char "
      ],
      "metadata": {
        "id": "ouReWEQn22af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('_______________________________________________________________________')\n",
        "print('_________________________Version 1(5 char)_____________________________')\n",
        "Tpos11, Tpos21, Tpos31 = accuracy(test_result_arab1, test_result_per1, test_result_kor1)\n",
        "\n",
        "try:\n",
        "  recall_arab1, recall_per1, recall_kor1 = recall(Tpos11, Tpos21, Tpos31)\n",
        "except:\n",
        "  print('RECALL:float division by zero')\n",
        "\n",
        "try:\n",
        "  precision_arab1, precision_per1, precision_kor1 = percision(test_result_arab1, test_result_per1, test_result_kor1)\n",
        "except:\n",
        "  print('PRECISION:float division by zero')\n",
        "\n",
        "print('F1-Measure:')\n",
        "try:\n",
        "  FMeasure_arab1 = fMeasure('Arabic', precision_arab1, recall_arab1)\n",
        "except:\n",
        "  print('Arabic:float division by zero')\n",
        "try:\n",
        "  FMeasure_per1 = fMeasure('Persain', precision_per1, recall_per1)\n",
        "except:\n",
        "  print('Persain:float division by zero')\n",
        "try:\n",
        "  FMeasure_kor1 = fMeasure('Kordish', precision_kor1, recall_kor1)\n",
        "except:\n",
        "  print('Kordish:float division by zero')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlqTYttM7Ewg",
        "outputId": "a6e8f1ce-2d94-43a5-dbb6-6c21d1f4d4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_______________________________________________________________________\n",
            "_________________________Version 1(5 char)_____________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 0.3333333333333333\n",
            "Accuracy for Persain is 0.3333333333333333\n",
            "Accuracy for Kordish is 0.3333333333333333\n",
            "Recall:\n",
            "Recall for Arabic is 0.0\n",
            "Recall for Persain is 0.0\n",
            "Recall for Kordish is 0.0\n",
            "Precision:\n",
            "Precision for Arabic is 0.0\n",
            "Precision for Persain is 0.0\n",
            "Precision for Kordish is 0.0\n",
            "F1-Measure:\n",
            "Arabic:float division by zero\n",
            "Persain:float division by zero\n",
            "Kordish:float division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 6.2.2 - Evaluate with Character Frequency (CF)"
      ],
      "metadata": {
        "id": "s5x8ODtv6dLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('_______________________________________________________________________')\n",
        "print('___________________________Version 2(CF)_______________________________')\n",
        "\n",
        "Tpos12, Tpos22, Tpos32 = accuracy(test_result_arab2, test_result_per2, test_result_kor2)\n",
        "\n",
        "try:\n",
        "  recall_arab2, recall_per2, recall_kor2 = recall(Tpos12, Tpos22, Tpos32)\n",
        "except:\n",
        "  print('RECALL:float division by zero')\n",
        "\n",
        "try:\n",
        "  precision_arab2, precision_per2, precision_kor2 = percision(test_result_arab2, test_result_per2, test_result_kor2)\n",
        "except:\n",
        "  print('PRECISION:float division by zero')\n",
        "\n",
        "print('F1-Measure:')\n",
        "try:\n",
        "  FMeasure_arab2 = fMeasure('Arabic', precision_arab2, recall_arab2)\n",
        "except:\n",
        "  print('Arabic:float division by zero')\n",
        "try:\n",
        "  FMeasure_per2 = fMeasure('Persain', precision_per2, recall_per2)\n",
        "except:\n",
        "  print('Persain:float division by zero')\n",
        "try:\n",
        "  FMeasure_kor2 = fMeasure('Kordish', precision_kor2, recall_kor2)\n",
        "except:\n",
        "  print('Kordish:float division by zero')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSGT_lQX7-RA",
        "outputId": "5be4e518-bb92-4691-c6ee-7aaa8f40c00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_______________________________________________________________________\n",
            "___________________________Version 2(CF)_______________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 1.0\n",
            "Accuracy for Persain is 1.0\n",
            "Accuracy for Kordish is 1.0\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 1.0\n",
            "Recall for Kordish is 1.0\n",
            "Precision:\n",
            "Precision for Arabic is 1.0\n",
            "Precision for Persain is 1.0\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 1.0\n",
            "F1-Measure for Persain is 1.0\n",
            "F1-Measure for Kordish is 1.0\n"
          ]
        }
      ]
    }
  ]
}